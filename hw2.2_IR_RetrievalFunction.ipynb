{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w0ieeEVXScqP"
   },
   "source": [
    "This notebook is used for SI 650 Information Retrieval class. You should implement retrieval functions and report corresponding results in your submission on Canvas. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "56RZ6Ya_SYng",
    "outputId": "3f3deb80-59de-485d-dfef-b440a6cbd2ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: metapy in /usr/local/lib/python2.7/dist-packages (0.2.13)\n"
     ]
    }
   ],
   "source": [
    "# install metapy, it may take several minutes.\n",
    "!pip install metapy\n",
    "import metapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "ey5kOfZiSd2_",
    "outputId": "78340d65-3045-455b-9b89-eaae59c45e4a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ‘lemur-stopwords.txt’ already there; not retrieving.\n",
      "\n",
      "--2018-10-18 21:38:32--  https://meta-toolkit.org/data/2016-11-10/cranfield.tar.gz\n",
      "Resolving meta-toolkit.org (meta-toolkit.org)... 50.116.41.177, 2600:3c02::f03c:91ff:feae:b777\n",
      "Connecting to meta-toolkit.org (meta-toolkit.org)|50.116.41.177|:443... connected.\n",
      "HTTP request sent, awaiting response... 304 Not Modified\n",
      "File ‘cranfield.tar.gz’ not modified on server. Omitting download.\n",
      "\n",
      "--2018-10-18 21:38:36--  http://www-personal.umich.edu/~shiyansi/cacm.tar.gz\n",
      "Resolving www-personal.umich.edu (www-personal.umich.edu)... 141.211.243.103\n",
      "Connecting to www-personal.umich.edu (www-personal.umich.edu)|141.211.243.103|:80... connected.\n",
      "HTTP request sent, awaiting response... 304 Not Modified\n",
      "File ‘cacm.tar.gz’ not modified on server. Omitting download.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reading Data\n",
    "!wget -nc https://raw.githubusercontent.com/meta-toolkit/meta/master/data/lemur-stopwords.txt\n",
    "!wget -N https://meta-toolkit.org/data/2016-11-10/cranfield.tar.gz\n",
    "!tar xf cranfield.tar.gz\n",
    "!wget -N http://www-personal.umich.edu/~shiyansi/cacm.tar.gz\n",
    "!tar xf cacm.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "orgxVgu0TMFF"
   },
   "outputs": [],
   "source": [
    "# Setting cranfield dataset\n",
    "with open('cranfield/tutorial.toml', 'w') as f:\n",
    "    f.write('type = \"line-corpus\"\\n')\n",
    "    f.write('store-full-text = true\\n')\n",
    "\n",
    "config = \"\"\"prefix = \".\" # tells MeTA where to search for datasets\n",
    "\n",
    "dataset = \"cranfield\" # a subfolder under the prefix directory\n",
    "corpus = \"tutorial.toml\" # a configuration file for the corpus specifying its format & additional args\n",
    "\n",
    "index = \"cranfield-idx\" # subfolder of the current working directory to place index files\n",
    "\n",
    "query-judgements = \"cranfield/cranfield-qrels.txt\" # file containing the relevance judgments for this dataset\n",
    "\n",
    "stop-words = \"lemur-stopwords.txt\"\n",
    "\n",
    "[[analyzers]]\n",
    "method = \"ngram-word\"\n",
    "ngram = 1\n",
    "filter = \"default-unigram-chain\"\n",
    "\"\"\"\n",
    "with open('cranfield-config.toml', 'w') as f:\n",
    "    f.write(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "myM1SK9zd0mt"
   },
   "outputs": [],
   "source": [
    "# Setting cacm dataset\n",
    "with open('cacm/tutorial.toml', 'w') as f:\n",
    "    f.write('type = \"line-corpus\"\\n')\n",
    "    f.write('store-full-text = true\\n')\n",
    "\n",
    "config = \"\"\"prefix = \".\" # tells MeTA where to search for datasets\n",
    "\n",
    "dataset = \"cacm\" # a subfolder under the prefix directory\n",
    "corpus = \"tutorial.toml\" # a configuration file for the corpus specifying its format & additional args\n",
    "\n",
    "index = \"cacm-idx\" # subfolder of the current working directory to place index files\n",
    "\n",
    "query-judgements = \"cacm/cacm-qrels.txt\" # file containing the relevance judgments for this dataset\n",
    "\n",
    "stop-words = \"lemur-stopwords.txt\"\n",
    "\n",
    "[[analyzers]]\n",
    "method = \"ngram-word\"\n",
    "ngram = 1\n",
    "filter = \"default-unigram-chain\"\n",
    "\"\"\"\n",
    "with open('cacm-config.toml', 'w') as f:\n",
    "    f.write(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3xWGLrMITjbA"
   },
   "outputs": [],
   "source": [
    "# Make sure you have installed metapy package and downloaded the data before running the following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fRtZ2uKbUBM9"
   },
   "outputs": [],
   "source": [
    "# Build the index for dataset.\n",
    "inv_idx_cran = metapy.index.make_inverted_index('cranfield-config.toml')\n",
    "inv_idx_cacm = metapy.index.make_inverted_index('cacm-config.toml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PKFJ8LFhYGMp"
   },
   "source": [
    "#** 3 Define New Retrieval Function**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I-_wqJFbh2a_"
   },
   "source": [
    "**Please write your own retrieval function in the cell below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lyI-On60YMsn"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "class  NewRF (metapy.index.RankingFunction):                                            \n",
    "                                                                       \n",
    "    def __init__(self, k1=2.2, b=0.9, k3=1000, c1=1, c2=6):\n",
    "      self.k1 = k1\n",
    "      self.b = b\n",
    "      self.k3 = k3\n",
    "      self.c1 = c1\n",
    "      self.c2 = c2\n",
    "      super(NewRF, self).__init__() \n",
    "\n",
    "        #recommended parameters from the literature: (k1=1.25, b=0.90, k3=1000)\n",
    "#         self.param = some_param\n",
    "\n",
    "#      super(NewRF, self).__init__()                                        \n",
    "                                                                                 \n",
    "    def score_one(self, sd):\n",
    "        \"\"\"\n",
    "        You need to override this function to return a score for a single term.\n",
    "        \n",
    "        You may want to call some of the following variables when implementing your retrieval function:\n",
    "        sd.avg_dl: average document length of the collection\n",
    "        sd.num_docs: total number of documents in the index\n",
    "        sd.total_terms: total number of terms in the index\n",
    "        sd.query_length: the total length of the current query (sum of all term weights)\n",
    "        sd.query_term_weight: query term count (or weight in case of feedback)\n",
    "        sd.doc_count: number of documents that a term t_id appears in\n",
    "        sd.corpus_term_count: number of times a term t_id appears in the collection\n",
    "        sd.doc_term_count: number of times the term appears in the current document\n",
    "        sd.doc_size: total number of terms in the current document\n",
    "        sd.doc_unique_terms: number of unique terms in the current document\n",
    "        \"\"\"\n",
    "        #Write your answer here collection\n",
    "        #(k1=1.25, b=0.90, k3=1000)\n",
    "        \n",
    "        \n",
    "        from math import log as log\n",
    "        from math import e as e\n",
    "        k1 = self.k1\n",
    "        b = self.b\n",
    "        k3 = self.k3\n",
    "        c1 = self.c1\n",
    "        c2 = self.c2\n",
    "        \n",
    "        # Proposed Algorithm BM25-CTF::\n",
    "        # BM25-CTF model = sum(bidf(w)) * (((k1 + 1) * btf(w,d))/(k1 *K(d) + btf(w,d)) * ((k3 +1) * btf(w,q)/(k3 +btf(w,q)))\n",
    "            # w = an index term (or word)\n",
    "            # d = doc as a collection of index terms\n",
    "           \n",
    "            \n",
    "        # K(d) = doc-length normalization factor from BM25 = \n",
    "        \n",
    "        # factor_one = bidf(w) = ictf(w) × pidf(w) × idf(w)\n",
    "        # M = total num terms in collection = log(num of docs/num docs term appears) = log(D/df(w)) = log(sd.num_docs/sd.doc_count)\n",
    "        # ctf(w) = collection term freq. = sd.corpus_term_count\n",
    "        # ictf(w) = inverse collec. term freq factor = log(M/ctf(w)) \n",
    "        \n",
    "        # Part 1 of the Equation: bidf_w\n",
    "        m = float(sd.total_terms)\n",
    "\n",
    "        ctf_w = float(sd.corpus_term_count) #num docs with a word\n",
    "        D = float(sd.num_docs)\n",
    "        df_w = float(sd.doc_count)\n",
    "    \n",
    "        #ictf_w\n",
    "        ictf_w = log(m / ctf_w, 10)\n",
    "      \n",
    "        # pidf_w = log((df_w_hat/ctf_w_hat) + 1)\n",
    "        df_w_hat = D * (1 - e**(-(ctf_w/D)))\n",
    "        ctf_w_hat = -D * log(1 - (df_w / D))\n",
    "        pidf_w = log((df_w_hat/ctf_w_hat) + 1, 10)\n",
    "        \n",
    "        # idf_w = -log(1 - e(-(ctf_w/|D|)))\n",
    "        idf_w = -log(1 - e**(-(ctf_w / D)))\n",
    "        \n",
    "        #factor_one = bidf_w\n",
    "        factor_one = ictf_w * pidf_w * idf_w\n",
    "        \n",
    "        \n",
    "        #Part 2 of the equation:\n",
    "        # Part 2 numerator = (k1+1) * btf_wd\n",
    "            # btf_wd = C(d) * (tf_wd/tf_wd_hat)\n",
    "#         tf_wd = sd.doc_term_count \n",
    "# #         ctf_wprime = 1\n",
    "#         tf_wd_hat = 1 + (1 * (D - sd.doc_unique_terms) )\n",
    "        \n",
    "#         tf_wpd =\n",
    "#         tf_wpd_hat = \n",
    "#         cd = D / (tf_wpd/tf_wpd_hat)\n",
    "#         btf_wd = cd * (tf_wd/tf_wd_hat)\n",
    "#         kd = \n",
    "        \n",
    "#         factor_two_numerator = (k1+1) * btf_wd\n",
    "#         factor_two_denominator = k1 * kd * btf_wd\n",
    "#         factor_two = factor_two_numerator/factor_two_denominator\n",
    "        \n",
    "        c_td = float(sd.doc_term_count)\n",
    "        d_size = float(sd.doc_size)\n",
    "        a_dl = float(sd.avg_dl)\n",
    "        \n",
    "        # create ddc, part_a=dd (document density), factor_one_a=ddc (document density)\n",
    "        part_a = ((c_td/D)/df_w)\n",
    "        factor_one_a = c1 * (log(part_a,10) + c2)\n",
    "   \n",
    "        # Part 2 of the equation\n",
    "        factor_two_numerator = (k1 + 1) * c_td\n",
    "        factor_two_denominator = (k1 * (1 - b + b *(d_size/a_dl))) + c_td\n",
    "        factor_two = factor_two_numerator/factor_two_denominator\n",
    "        \n",
    "        \n",
    "        #Part 3 of the equation\n",
    "        factor_three_numerator = (k3 + 1) * sd.query_term_weight\n",
    "        factor_three_denominator = (k3 + sd.query_term_weight)\n",
    "        factor_three = factor_three_numerator / factor_three_denominator \n",
    "        \n",
    "        result = (factor_one + factor_one_a) * factor_two * factor_three\n",
    "# \n",
    "        \n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wOaLpCJdYRqP"
   },
   "outputs": [],
   "source": [
    "ranker = NewRF()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "50AaAT1_uvVa"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "VoosxShIYRyv",
    "outputId": "8d4732fa-b414-4339-c716-c0bbefc36b36"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP 0.290234156138\n",
      "Precision@30 0.124888888889\n"
     ]
    }
   ],
   "source": [
    "ev = metapy.index.IREval('cranfield-config.toml')\n",
    "num_results = 30\n",
    "\n",
    "precision_list = []\n",
    "with open('cranfield/cranfield-queries.txt') as query_file:\n",
    "    for query_num, line in enumerate(query_file):\n",
    "        query = metapy.index.Document()\n",
    "        query.content(line.strip())\n",
    "        results = ranker.score(inv_idx_cran, query, num_results)                            \n",
    "        avg_p = ev.avg_p(results, query_num + 1, num_results)\n",
    "        precision_list.append(ev.precision(results,query_num+1,num_results))\n",
    "\n",
    "print \"MAP\", ev.map()\n",
    "print \"Precision@30\", sum(precision_list) / len(precision_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "AGEg2h94fyBH",
    "outputId": "392c2230-144b-47b8-c600-2d74763da225"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP 0.238704532083\n",
      "Precision@30 0.1703125\n"
     ]
    }
   ],
   "source": [
    "ev2 = metapy.index.IREval('cacm-config.toml')\n",
    "num_results = 30\n",
    "\n",
    "precision_list = []\n",
    "with open('cacm/cacm-queries.txt') as query_file:\n",
    "    for query_num, line in enumerate(query_file):\n",
    "        query = metapy.index.Document()\n",
    "        query.content(line.strip())\n",
    "        results = ranker.score(inv_idx_cacm, query, num_results)                            \n",
    "        avg_p = ev2.avg_p(results, query_num + 1, num_results)\n",
    "        precision_list.append(ev2.precision(results, query_num+1, num_results))\n",
    "\n",
    "print \"MAP\", ev2.map()\n",
    "print \"Precision@30\", sum(precision_list) / len(precision_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W3SwUDEW1l9Z"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D3eCOuatYSa8"
   },
   "source": [
    "# Testing Search Results for a Single Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pY8niuLdYZs2"
   },
   "outputs": [],
   "source": [
    "query = metapy.index.Document()\n",
    "query.content(\"ibm\")\n",
    "top_docs = ranker.score(inv_idx_cacm, query, num_results=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 207
    },
    "colab_type": "code",
    "id": "YQpQX0bQYdIh",
    "outputId": "3ad7e691-2dc4-4170-b91d-ce0954cab6c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. ibm 704 code nundrums...\n",
      "\n",
      "2. character scanning on the ibm 7070...\n",
      "\n",
      "3. counting ones on the ibm 7090...\n",
      "\n",
      "4. statistical programs for the ibm 650 part i a collection is given of brief descriptions of statistical programs now in use in university computing centers which have ibm 650...\n",
      "\n",
      "5. realizing boolean connectives on the ibm 1620...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for num, (d_id, _) in enumerate(top_docs):\n",
    "    content = inv_idx_cacm.metadata(d_id).get('content')\n",
    "    print(\"{}. {}...\\n\".format(num + 1, content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KDqQ0j-oignb"
   },
   "source": [
    "**Please submit your code for  NewRF class to canvas. We need your code to verify your results.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rPRofDpZRRdf"
   },
   "source": [
    "**Reasons for choosing the models I chose**:\n",
    "\n",
    "For this assignment, for the article, I searched the U of M library search engine. My query was BM25 and among the top articles (or the 4th article) that was retrieved in the Articles section was the article, *BM25-CTF: Improving TF and IDF factors in BM25 by using collection term frequencies*. In the article, it states that BM25-CTF imporves BM25 by 17.6% for the TREC dataset and 7.6% averaged across all collections included in the test. This is the main reason for trying this algorithm. Looking within the article, it seems that they found other algorithms that beat the Okapi BM25 algorithm, however, the BM25-CTF seems to perform better than those.  The only downfall of using this algorithm is that when the collection documents are not as long, the BM25-CTF will not perform as well. \n",
    "\n",
    "To further improve the BM25-CTF algorithm, I added parameters suggested in the article, *Word Document Density and Relevance Scoring*. The parameters they suggested were ddc  and dd(which is within ddc). This parameter helps to capture the quantity of word repetitiveness and is added to idf in BM25. Adding this parameter helped raise the MAP score for the Cranfield dataset - beating the BM25 score, but not the CACM dataset. \n",
    "\n",
    "The recommended values for k1, b, and k3 were as follows: (k1=1.25, b=0.90, k3=1000). Since the k3 values do not matter as much for this equation, I changed the b and k1 values to make my model run better. I found when k1=1.2, b=0.9, k3=100 it improved my MAP scores for the Cranfield and CACM data. For the c1 and c2 values, I looked at the values retrieval performance for average precision on TREC and the best performance seemed to be at c1=1, c2=6.\n",
    "\n",
    "**Articles used**:\n",
    "\n",
    "Jimenez, Sergio, Silviu-Petru Cucerzan, Fabio A. Gonzalez, Alexander Gelbukh, and George Duenas, *BM25-CTF: Improving TF and IDF factors in BM25 by using collection term frequencies*. Journal of Intelligent & Fuzzy Systems 34, 2018, pp.2887–2899.\n",
    "\n",
    "Frans, Martin and J. Scott McCarley, *Word Document Density and Relevance Scoring*. In *Proceedings of the 23rd ACM SIGIR*, 2000, pp.345-347."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nZT76HDoh0sM"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Virtual_IR_Lab_AS2_2.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
